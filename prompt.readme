# TELEMATICS-BASED AUTO INSURANCE SYSTEM - IMPLEMENTATION PROMPT

## PROJECT OVERVIEW

You are tasked with building a production-ready telematics-based automobile insurance system that enables usage-based insurance (UBI) pricing models. This system will collect real-time driving behavior data, process it through distributed streaming and batch pipelines, calculate risk scores using machine learning, and provide dynamic insurance pricing.

## BUSINESS CONTEXT

### Problem Statement
Traditional auto insurance pricing relies on generalized demographic factors (age, location, vehicle type) that don't reflect actual driving behavior. This results in:
- Unfair premiums for safe drivers
- No incentive for safer driving habits
- Limited customer engagement and transparency

### Solution
Implement a telematics solution that:
1. Collects real-time vehicle and driver behavior data (speed, braking, acceleration, location, time of travel)
2. Processes data through streaming and batch pipelines
3. Calculates personalized risk scores using machine learning
4. Dynamically adjusts insurance premiums based on actual driving behavior
5. Provides transparent dashboards showing driving scores and premium impacts

### Business Goals
- **Improve premium accuracy** by 30-40% compared to traditional models
- **Reduce claims** by incentivizing safer driving (target: 15-20% reduction)
- **Increase customer satisfaction** through transparency and fairness
- **Achieve ROI** within 18-24 months of deployment

## TECHNICAL ARCHITECTURE

### System Architecture Overview

```
Data Sources (Telematics + External APIs)
    ‚Üì
Apache Kafka (Message Broker)
    ‚Üì
Stream Processing (Spark Structured Streaming)
    ‚Üì                           ‚Üì
Feature Store (Redis)    Data Lake (S3/Delta Lake)
    ‚Üì                           ‚Üì
ML Pipeline (XGBoost)    Batch Processing (Airflow + Spark)
    ‚Üì                           ‚Üì
API Layer (FastAPI)      ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
User Dashboard (React)
```

### Technology Stack

**Data Ingestion:**
- Apache Kafka 3.6+ (message broker)
- Confluent Schema Registry (Avro schemas)
- Kafka Connect (external data integration)

**Stream Processing:**
- Apache Spark 3.5+ Structured Streaming
- Alternative: Apache Flink 1.18+ (for more complex event processing)

**Batch Processing:**
- Apache Airflow 2.8+ (orchestration)
- PySpark (distributed data processing)

**Storage:**
- PostgreSQL 15+ (transactional data, user profiles)
- Redis 7+ (feature store, caching)
- S3 + Delta Lake (data lake for historical data)
- MongoDB (optional: for flexible document storage)

**Machine Learning:**
- XGBoost/LightGBM (risk scoring models)
- scikit-learn (preprocessing, evaluation)
- MLflow (experiment tracking, model registry)
- SHAP (model explainability)

**API & Backend:**
- FastAPI 0.109+ (REST API)
- Pydantic (data validation)
- Celery (async task queue)

**Frontend:**
- React 18+ (dashboard)
- Recharts/Chart.js (data visualization)
- Tailwind CSS (styling)

**Infrastructure:**
- Docker & Docker Compose (local development)
- Kubernetes (production deployment - optional for POC)
- Terraform (infrastructure as code)
- AWS/GCP/Azure (cloud provider)

**Monitoring & Observability:**
- Prometheus (metrics)
- Grafana (dashboards)
- ELK Stack (logging)
- Sentry (error tracking)

## DETAILED REQUIREMENTS

### 1. DATA COLLECTION & INGESTION

#### 1.1 Telematics Data Schema

Create Avro schema for telematics events:

```json
{
  "type": "record",
  "name": "TelematicsEvent",
  "namespace": "com.insurance.telematics",
  "fields": [
    {"name": "event_id", "type": "string"},
    {"name": "device_id", "type": "string"},
    {"name": "driver_id", "type": "string"},
    {"name": "timestamp", "type": "long", "logicalType": "timestamp-millis"},
    {"name": "latitude", "type": "double"},
    {"name": "longitude", "type": "double"},
    {"name": "speed", "type": "double", "doc": "Speed in mph"},
    {"name": "acceleration", "type": "double", "doc": "g-force"},
    {"name": "braking_force", "type": ["null", "double"], "default": null},
    {"name": "heading", "type": "double", "doc": "Compass heading 0-360"},
    {"name": "altitude", "type": ["null", "double"], "default": null},
    {"name": "gps_accuracy", "type": "double", "doc": "GPS accuracy in meters"},
    {"name": "event_type", "type": ["null", "string"], "default": null},
    {"name": "trip_id", "type": ["null", "string"], "default": null}
  ]
}
```

#### 1.2 Data Simulation Requirements

Create a realistic telematics data simulator with:

**Trip Generation:**
- Generate 1000 simulated drivers
- Each driver: 2-4 trips per day
- Trip duration: 5-60 minutes
- Mileage distribution: Normal distribution (Œº=12000, œÉ=4000 annual miles)

**Driving Behavior Profiles:**
1. **Safe Driver (40% of population):**
   - Speed: 95% within speed limit, max 5-10 mph over
   - Harsh braking: 0-2 per 100 miles
   - Rapid acceleration: 0-1 per 100 miles
   - Night driving: <10% of miles

2. **Average Driver (45% of population):**
   - Speed: 80% within limit, occasional 10-15 mph over
   - Harsh braking: 2-5 per 100 miles
   - Rapid acceleration: 2-4 per 100 miles
   - Night driving: 10-20% of miles

3. **Risky Driver (15% of population):**
   - Speed: frequent speeding, 15-25 mph over
   - Harsh braking: 5-10+ per 100 miles
   - Rapid acceleration: 5-8+ per 100 miles
   - Night driving: 20-30% of miles

**Realistic Patterns:**
- Commute patterns: peak hours (7-9am, 5-7pm) on weekdays
- Geographic variation: urban vs suburban vs highway
- Weather correlation: reduce speed in rain/snow
- Time of day effects: faster at night, slower in traffic
- Weekend behavior: more leisure trips, different times

**Event Types to Generate:**
- `normal`: regular driving
- `harsh_brake`: braking force > 0.4g
- `rapid_accel`: acceleration > 0.35g
- `speeding`: speed > limit + 15 mph
- `harsh_corner`: lateral acceleration > 0.3g
- `phone_usage`: (random, 5-15% of trips for some drivers)

#### 1.3 External Data Integration

Implement connectors for:

1. **Weather Data:**
   - Source: OpenWeatherMap API or NOAA
   - Enrich trips with weather conditions
   - Fields: temperature, precipitation, visibility, road conditions

2. **Traffic Incidents:**
   - Source: Local DOT APIs or TomTom Traffic API
   - Correlate with driver location/time
   - Fields: incident type, severity, location, duration

3. **Crime Statistics:**
   - Source: FBI Crime Data API or local police APIs
   - Map to ZIP codes or geo-boundaries
   - Update: monthly batch process

4. **DMV Records (Simulated):**
   - Driver's license info
   - Ticket history
   - Accident history
   - License suspensions

#### 1.4 Kafka Topic Design

Create the following topics with appropriate partitioning:

```
telematics-raw (12 partitions)
‚îú‚îÄ‚îÄ Key: device_id
‚îî‚îÄ‚îÄ Retention: 7 days

telematics-enriched (12 partitions)
‚îú‚îÄ‚îÄ Key: driver_id
‚îî‚îÄ‚îÄ Retention: 30 days

risk-events (6 partitions)
‚îú‚îÄ‚îÄ Key: driver_id
‚îî‚îÄ‚îÄ Retention: 90 days

trip-aggregated (6 partitions)
‚îú‚îÄ‚îÄ Key: driver_id
‚îî‚îÄ‚îÄ Retention: 365 days (compacted)

external-data-weather (3 partitions)
external-data-traffic (3 partitions)
external-data-crime (1 partition)
```

### 2. STREAM PROCESSING

#### 2.1 Real-Time Event Detection

Implement Spark Structured Streaming job:

**Requirements:**
- Process events with <5 second latency
- Detect and flag risk events in real-time
- Calculate windowed aggregations (1-min, 5-min, 15-min windows)
- Maintain state for trip sessions
- Handle late-arriving data (10-minute watermark)

**Key Transformations:**
1. **Event Classification:**
   ```python
   def classify_event(speed, acceleration, braking_force, speed_limit):
       if braking_force > 0.4:
           return 'harsh_brake'
       elif acceleration > 0.35:
           return 'rapid_accel'
       elif speed > speed_limit + 15:
           return 'speeding'
       else:
           return 'normal'
   ```

2. **Trip Sessionization:**
   - Group events into trips based on 10-minute inactivity gaps
   - Calculate trip-level metrics: duration, distance, avg speed, risk events

3. **Real-Time Aggregations:**
   - Per-driver rolling 24-hour metrics
   - Per-driver rolling 7-day metrics
   - Per-driver rolling 30-day metrics

4. **Anomaly Detection:**
   - Flag sudden changes in driving behavior
   - Detect potential device tampering or data quality issues

#### 2.2 Feature Store Updates

Write to Redis with the following key patterns:

```
driver:{driver_id}:realtime
‚îú‚îÄ‚îÄ last_trip_timestamp
‚îú‚îÄ‚îÄ last_24h_harsh_brakes
‚îú‚îÄ‚îÄ last_24h_rapid_accels
‚îú‚îÄ‚îÄ last_24h_speeding_events
‚îú‚îÄ‚îÄ last_24h_miles
‚îî‚îÄ‚îÄ last_24h_avg_speed

driver:{driver_id}:7day
‚îú‚îÄ‚îÄ total_miles
‚îú‚îÄ‚îÄ harsh_brakes_per_100mi
‚îú‚îÄ‚îÄ speeding_incidents
‚îú‚îÄ‚îÄ night_driving_pct
‚îî‚îÄ‚îÄ avg_trip_duration

driver:{driver_id}:30day
‚îú‚îÄ‚îÄ total_miles
‚îú‚îÄ‚îÄ total_trips
‚îú‚îÄ‚îÄ risk_score
‚îî‚îÄ‚îÄ risk_trend
```

#### 2.3 Data Quality Checks

Implement streaming quality checks:
- GPS accuracy validation
- Speed reasonableness checks (< 150 mph)
- Temporal consistency (no future timestamps)
- Device heartbeat monitoring
- Duplicate event detection

### 3. BATCH PROCESSING

#### 3.1 Airflow DAG Structure

Create the following DAGs:

**DAG 1: daily_trip_processing**
```
Schedule: @daily (2:00 AM)
Tasks:
1. aggregate_trips_from_raw_events
2. calculate_trip_metrics
3. enrich_with_weather_data
4. enrich_with_traffic_data
5. detect_trip_patterns (commute vs leisure)
6. write_to_data_lake
7. update_driver_profiles
8. data_quality_checks
```

**DAG 2: weekly_risk_scoring**
```
Schedule: 0 3 * * 0 (Sunday 3:00 AM)
Tasks:
1. extract_weekly_features
2. calculate_risk_scores_batch
3. compare_with_previous_week
4. generate_risk_alerts
5. update_feature_store
6. update_pricing_recommendations
```

**DAG 3: monthly_model_training**
```
Schedule: 0 2 1 * * (First day of month, 2:00 AM)
Tasks:
1. extract_training_data (last 6 months)
2. feature_engineering
3. train_test_split
4. train_risk_model
5. evaluate_model_performance
6. compare_with_production_model
7. register_model_if_better (MLflow)
8. generate_model_report
```

**DAG 4: external_data_refresh**
```
Schedule: @daily (1:00 AM)
Tasks:
1. fetch_weather_forecasts
2. fetch_traffic_incidents
3. update_crime_statistics (monthly)
4. validate_external_data
5. load_to_staging
6. update_production_tables
```

#### 3.2 Feature Engineering (Batch)

Comprehensive feature set to generate:

**Behavioral Features (per driver):**
```python
features = {
    # Speed metrics
    'avg_speed_overall': float,
    'avg_speed_highway': float,
    'avg_speed_urban': float,
    'max_speed_recorded': float,
    'speed_variance': float,
    'speeding_incidents_per_100mi': float,
    
    # Braking/Acceleration
    'harsh_braking_rate': float,  # per 100 miles
    'rapid_accel_rate': float,
    'smooth_driving_score': float,  # 0-100
    
    # Time patterns
    'night_driving_pct': float,  # 10pm-5am
    'rush_hour_driving_pct': float,
    'weekend_driving_pct': float,
    'late_night_driving_pct': float,  # 12am-4am
    
    # Trip patterns
    'avg_trip_duration_min': float,
    'avg_trip_distance_miles': float,
    'total_trips_per_month': int,
    'commute_pattern_detected': bool,
    'trip_consistency_score': float,  # regularity
    
    # Location risk
    'urban_miles_pct': float,
    'highway_miles_pct': float,
    'high_crime_area_miles_pct': float,
    'accident_prone_route_frequency': float,
    
    # Weather correlation
    'bad_weather_driving_pct': float,
    'rain_driving_caution_score': float,
    
    # Phone usage (if available)
    'distracted_driving_incidents': int,
    'phone_free_trips_pct': float,
    
    # Historical
    'total_miles_ytd': float,
    'months_of_data': int,
    'data_completeness_score': float
}
```

**Demographic Features:**
```python
demographics = {
    'age': int,
    'years_licensed': int,
    'gender': str,
    'zip_code': str,
    'vehicle_age': int,
    'vehicle_make': str,
    'vehicle_model': str,
    'vehicle_safety_rating': int,  # 1-5 stars
    
    # Historical claims/tickets
    'prior_claims_3yr': int,
    'prior_tickets_3yr': int,
    'prior_accidents_3yr': int,
    'license_suspensions': int,
    
    # Traditional risk factors
    'years_at_current_address': int,
    'credit_score_band': str,  # if legally allowed
    'marital_status': str
}
```

**Time-Based Features:**
```python
temporal_features = {
    'days_since_last_incident': int,
    'days_since_last_trip': int,
    'risk_score_7d_change': float,
    'risk_score_30d_change': float,
    'mileage_trend': str,  # increasing/stable/decreasing
    'behavior_improvement_trend': float
}
```

#### 3.3 Data Lake Schema

Organize data in S3 with the following structure:

```
s3://telematics-data-lake/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ telematics/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ year=2024/month=11/day=08/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ part-00000.parquet
‚îÇ   ‚îú‚îÄ‚îÄ weather/
‚îÇ   ‚îú‚îÄ‚îÄ traffic/
‚îÇ   ‚îî‚îÄ‚îÄ crime/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ trips/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ year=2024/month=11/day=08/
‚îÇ   ‚îú‚îÄ‚îÄ daily_aggregates/
‚îÇ   ‚îî‚îÄ‚îÄ weekly_aggregates/
‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îú‚îÄ‚îÄ driver_features_snapshot/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date=2024-11-08/
‚îÇ   ‚îî‚îÄ‚îÄ training_datasets/
‚îÇ       ‚îî‚îÄ‚îÄ version=v1.2/
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ risk_scoring/
    ‚îÇ   ‚îî‚îÄ‚îÄ version=v1.0/
    ‚îî‚îÄ‚îÄ pricing/
```

Use Delta Lake for ACID transactions and time travel capabilities.

### 4. MACHINE LEARNING PIPELINE

#### 4.1 Risk Scoring Model

**Model Architecture: Gradient Boosted Trees (XGBoost/LightGBM)**

**Justification:**
- Best for tabular data with mixed feature types
- Interpretable via SHAP values (regulatory requirement)
- Fast inference (<10ms per prediction)
- Robust to missing data
- Excellent performance on insurance risk datasets

**Target Variable Options:**

Option 1 - **Risk Score (Regression):**
```
Target: risk_score (0-100 scale)
Derived from: weighted combination of
- Incident rate (harsh events per 100 miles)
- Severity of incidents
- Time-risk patterns (night/weather)
- Historical claims data (if available)
```

Option 2 - **Claim Probability (Binary Classification):**
```
Target: will_file_claim_6_months (0/1)
Requires: Historical claims data for training
```

Option 3 - **Risk Category (Multi-class Classification):**
```
Target: risk_tier (0=Excellent, 1=Good, 2=Average, 3=Below Average, 4=High Risk)
Derived from: percentile ranking of risk metrics
```

**Recommended Approach: Start with Risk Score (Regression)**

#### 4.2 Model Training Pipeline

```python
# Pseudocode for training pipeline

class RiskScoringPipeline:
    
    def __init__(self):
        self.feature_columns = None
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = {}
        
    def prepare_training_data(self, start_date, end_date):
        """
        Extract training data from data lake
        - Last 6-12 months of driving data
        - Minimum 500 miles per driver
        - At least 30 days of data per driver
        """
        query = """
        SELECT 
            driver_id,
            -- behavioral features
            avg_speed, max_speed, speed_variance,
            harsh_braking_per_100mi,
            rapid_accel_per_100mi,
            speeding_incidents_per_100mi,
            -- time patterns
            night_driving_pct,
            rush_hour_pct,
            weekend_driving_pct,
            -- location risk
            urban_driving_pct,
            high_crime_area_pct,
            -- demographic
            age, years_licensed,
            vehicle_age, vehicle_safety_rating,
            prior_claims_3yr, prior_tickets_3yr,
            -- target
            incident_score as target
        FROM driver_features_snapshot
        WHERE snapshot_date BETWEEN :start_date AND :end_date
            AND total_miles >= 500
            AND days_of_data >= 30
        """
        df = spark.sql(query).toPandas()
        return df
    
    def create_risk_labels(self, df):
        """
        Create risk score target from driving metrics
        Risk Score = weighted sum of normalized metrics
        """
        weights = {
            'harsh_braking_per_100mi': 0.25,
            'rapid_accel_per_100mi': 0.15,
            'speeding_incidents_per_100mi': 0.20,
            'night_driving_pct': 0.10,
            'max_speed': 0.15,
            'prior_claims_3yr': 0.10,
            'prior_tickets_3yr': 0.05
        }
        
        risk_score = 0
        for feature, weight in weights.items():
            # Normalize to 0-1, then scale by weight
            normalized = (df[feature] - df[feature].min()) / (df[feature].max() - df[feature].min())
            risk_score += normalized * weight * 100
        
        return risk_score.clip(0, 100)
    
    def train(self, X_train, y_train, X_val, y_val):
        """
        Train XGBoost model with hyperparameter tuning
        """
        import xgboost as xgb
        from sklearn.model_selection import RandomizedSearchCV
        
        param_distributions = {
            'max_depth': [3, 4, 5, 6, 7],
            'learning_rate': [0.01, 0.05, 0.1, 0.2],
            'n_estimators': [100, 200, 300],
            'subsample': [0.6, 0.7, 0.8, 0.9],
            'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
            'min_child_weight': [1, 3, 5],
            'gamma': [0, 0.1, 0.2]
        }
        
        xgb_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            random_state=42
        )
        
        random_search = RandomizedSearchCV(
            xgb_model,
            param_distributions,
            n_iter=50,
            cv=5,
            scoring='neg_mean_squared_error',
            n_jobs=-1,
            random_state=42
        )
        
        random_search.fit(X_train, y_train)
        self.model = random_search.best_estimator_
        
        # Log to MLflow
        import mlflow
        mlflow.log_params(random_search.best_params_)
        mlflow.log_metric('train_rmse', 
                         np.sqrt(-random_search.best_score_))
        
        return self.model
    
    def evaluate(self, X_test, y_test):
        """
        Comprehensive model evaluation
        """
        from sklearn.metrics import (
            mean_squared_error, mean_absolute_error,
            r2_score, explained_variance_score
        )
        
        predictions = self.model.predict(X_test)
        
        metrics = {
            'rmse': np.sqrt(mean_squared_error(y_test, predictions)),
            'mae': mean_absolute_error(y_test, predictions),
            'r2': r2_score(y_test, predictions),
            'explained_variance': explained_variance_score(y_test, predictions)
        }
        
        # Evaluate by risk bucket
        y_test_buckets = pd.cut(y_test, bins=[0, 20, 40, 60, 80, 100], 
                                labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        pred_buckets = pd.cut(predictions, bins=[0, 20, 40, 60, 80, 100],
                             labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])
        
        from sklearn.metrics import classification_report
        bucket_accuracy = classification_report(y_test_buckets, pred_buckets)
        
        return metrics, bucket_accuracy
    
    def explain_predictions(self, X_sample):
        """
        Generate SHAP explanations for regulatory compliance
        """
        import shap
        
        explainer = shap.TreeExplainer(self.model)
        shap_values = explainer.shap_values(X_sample)
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': X_sample.columns,
            'importance': np.abs(shap_values).mean(axis=0)
        }).sort_values('importance', ascending=False)
        
        return explainer, shap_values, feature_importance
```

#### 4.3 Model Evaluation Criteria

**Performance Metrics:**
- RMSE < 10 (on 0-100 scale)
- R¬≤ > 0.70
- MAE < 7.5
- Bucket classification accuracy > 75%

**Fairness Metrics:**
- Demographic parity: Check for bias across protected classes
- Equal opportunity: Similar false positive/negative rates
- Calibration: Predicted vs actual risk alignment

**Business Metrics:**
- Correlation with actual claims: > 0.60
- Premium adjustment range: 70-150% of base premium
- Customer retention impact: Monitor churn in high-risk tier

#### 4.4 Model Monitoring

Implement continuous monitoring:
- Prediction drift detection
- Feature drift detection
- Model performance degradation alerts
- A/B testing framework for model versions

### 5. PRICING ENGINE

#### 5.1 Premium Calculation Logic

Implement a hybrid pricing model:

```
Final_Premium = Base_Premium √ó Risk_Multiplier √ó Usage_Multiplier √ó Discount_Factor √ó Regional_Factor
```

**Components:**

1. **Base Premium (Traditional Model):**
```python
def calculate_base_premium(driver_profile, vehicle_profile, location):
    """
    Traditional actuarial model based on:
    - Demographics (age, gender, marital status)
    - Vehicle (make, model, age, safety rating)
    - Location (ZIP code risk rating)
    - Coverage level
    """
    base_rate = REGIONAL_BASE_RATES[location.zip_code]
    
    # Age factor
    age_factor = AGE_MULTIPLIERS.get(driver_profile.age_band, 1.0)
    
    # Vehicle factor
    vehicle_factor = (
        VEHICLE_TYPE_MULTIPLIERS[vehicle_profile.type] *
        VEHICLE_AGE_MULTIPLIERS[vehicle_profile.age_band] *
        VEHICLE_SAFETY_MULTIPLIERS[vehicle_profile.safety_rating]
    )
    
    # Historical factor
    history_factor = (
        1.0 + 
        driver_profile.claims_3yr * 0.25 +
        driver_profile.tickets_3yr * 0.15
    )
    
    return base_rate * age_factor * vehicle_factor * history_factor
```

2. **Risk Multiplier (Telematics-Based):**
```python
def calculate_risk_multiplier(risk_score):
    """
    Map risk score (0-100) to premium multiplier
    - Excellent driver (0-20): 0.70x (30% discount)
    - Good driver (20-40): 0.85x (15% discount)
    - Average driver (40-60): 1.00x (no change)
    - Below average (60-80): 1.20x (20% surcharge)
    - High risk (80-100): 1.50x (50% surcharge)
    """
    if risk_score <= 20:
        return 0.70
    elif risk_score <= 40:
        return 0.85
    elif risk_score <= 60:
        return 1.00
    elif risk_score <= 80:
        return 1.20
    else:
        return 1.50
```

3. **Usage Multiplier (PAYD Component):**
```python
def calculate_usage_multiplier(annual_mileage):
    """
    Adjust for actual miles driven
    Baseline: 12,000 miles/year
    """
    baseline = 12000
    if annual_mileage < 5000:
        return 0.75  # Very low mileage
    elif annual_mileage < baseline:
        return 0.80 + (annual_mileage / baseline) * 0.20
    elif annual_mileage < 15000:
        return 1.00
    elif annual_mileage < 20000:
        return 1.15
    else:
        return 1.30  # High mileage
```

4. **Discount Factor (Incentives):**
```python
def calculate_discount_factor(driver_metrics):
    """
    Additional discounts for positive behaviors
    """
    discount = 0.0
    
    # Consistency bonus
    if driver_metrics.data_completeness > 0.95:
        discount += 0.02  # 2% for complete data
    
    # Improvement bonus
    if driver_metrics.risk_score_trend < -5:  # improving
        discount += 0.03  # 3% for improvement
    
    # Defensive driving course
    if driver_metrics.completed_safety_course:
        discount += 0.05  # 5% for training
    
    # Safe device installation
    if driver_metrics.device_active_days > 90:
        discount += 0.02  # 2% for sustained participation
    
    return max(1.0 - discount, 0.70)  # Cap total discount
```

#### 5.2 Dynamic Pricing Rules

Implement business rules:

- **Update Frequency:** Monthly recalculation
- **Maximum Adjustment:** ¬±30% per period (to prevent shock)
- **Minimum Data Requirement:** 500 miles or 30 days
- **Grace Period:** 60 days for new customers (trial period)
- **Notification:** 30-day advance notice for premium increases
- **Appeal Process:** Allow customers to contest scores

#### 5.3 Pricing API Endpoints

Required endpoints:

```
POST /api/v1/pricing/calculate
GET  /api/v1/pricing/{driver_id}/current
GET  /api/v1/pricing/{driver_id}/projected
GET  /api/v1/pricing/{driver_id}/history
POST /api/v1/pricing/{driver_id}/simulate (what-if analysis)
```

### 6. API LAYER

#### 6.1 FastAPI Application Structure

```
app/
‚îú‚îÄ‚îÄ main.py                 # FastAPI app initialization
‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îú‚îÄ‚îÄ dependencies.py        # Dependency injection
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ schemas.py        # Pydantic models
‚îÇ   ‚îî‚îÄ‚îÄ database.py       # SQLAlchemy models
‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îú‚îÄ‚îÄ telematics.py     # Telematics endpoints
‚îÇ   ‚îú‚îÄ‚îÄ drivers.py        # Driver management
‚îÇ   ‚îú‚îÄ‚îÄ risk.py           # Risk scoring endpoints
‚îÇ   ‚îú‚îÄ‚îÄ pricing.py        # Pricing endpoints
‚îÇ   ‚îî‚îÄ‚îÄ analytics.py      # Analytics/reporting
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ kafka_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ redis_client.py
‚îÇ   ‚îú‚îÄ‚îÄ risk_scoring_service.py
‚îÇ   ‚îî‚îÄ‚îÄ pricing_service.py
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py
‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering.py
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ auth.py           # JWT authentication
    ‚îú‚îÄ‚îÄ rate_limiter.py
    ‚îî‚îÄ‚îÄ validators.py
```

#### 6.2 Core API Endpoints

**Authentication:**
```
POST /api/v1/auth/login
POST /api/v1/auth/refresh
POST /api/v1/auth/logout
```

**Telematics Ingestion:**
```
POST   /api/v1/telematics/events          # Ingest single event
POST   /api/v1/telematics/events/batch    # Batch ingest
GET    /api/v1/telematics/devices/{id}/health
```

**Driver Management:**
```
POST   /api/v1/drivers                    # Register new driver
GET    /api/v1/drivers/{id}               # Get driver profile
PATCH  /api/v1/drivers/{id}               # Update profile
GET    /api/v1/drivers/{id}/trips         # Get trip history
GET    /api/v1/drivers/{id}/statistics    # Driving statistics
```

**Risk Scoring:**
```
GET    /api/v1/risk/{driver_id}/score           # Current risk score
GET    /api/v1/risk/{driver_id}/breakdown       # Score breakdown with SHAP
GET    /api/v1/risk/{driver_id}/history         # Historical scores
GET    /api/v1/risk/{driver_id}/recommendations # Improvement tips
```

**Pricing:**
```
GET    /api/v1/pricing/{driver_id}/current      # Current premium
GET    /api/v1/pricing/{driver_id}/breakdown    # Premium components
POST   /api/v1/pricing/{driver_id}/simulate     # What-if scenarios
GET    /api/v1/pricing/{driver_id}/comparison   # Compare with traditional
```

**Analytics:**
```
GET    /api/v1/analytics/fleet/summary          # Fleet-wide metrics
GET    /api/v1/analytics/risk-distribution      # Risk distribution
GET    /api/v1/analytics/savings                # Program savings
```

#### 6.3 Request/Response Models

Use Pydantic for validation:

```python
from pydantic import BaseModel, Field, validator
from datetime import datetime
from typing import Optional, List

class TelematicsEventRequest(BaseModel):
    device_id: str
    timestamp: datetime
    latitude: float = Field(..., ge=-90, le=90)
    longitude: float = Field(..., ge=-180, le=180)
    speed: float = Field(..., ge=0, le=150)
    acceleration: float
    braking_force: Optional[float] = None
    
    @validator('speed')
    def speed_reasonable(cls, v):
        if v > 150:
            raise ValueError('Speed exceeds maximum reasonable value')
        return v

class RiskScoreResponse(BaseModel):
    driver_id: str
    risk_score: float
    risk_category: str
    confidence: float
    calculated_at: datetime
    factors: List[dict]  # SHAP values
    recommendations: List[str]

class PremiumBreakdown(BaseModel):
    base_premium: float
    risk_multiplier: float
    usage_multiplier: float
    discount_factor: float
    final_premium: float
    monthly_premium: float
    savings_vs_traditional: float
    effective_date: datetime
```

#### 6.4 Authentication & Authorization

Implement JWT-based auth:

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from datetime import datetime, timedelta

SECRET_KEY = "your-secret-key"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

security = HTTPBearer()

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        return user_id
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
```

#### 6.5 Rate Limiting

Implement rate limiting for API protection:

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/api/v1/telematics/events")
@limiter.limit("1000/minute")  # High limit for telematics
async def ingest_event(request: Request, event: TelematicsEventRequest):
    # Process event
    pass

@app.get("/api/v1/risk/{driver_id}/score")
@limiter.limit("60/minute")  # Lower limit for analytics
async def get_risk_score(request: Request, driver_id: str):
    # Return risk score
    pass
```

### 7. USER DASHBOARD

#### 7.1 Dashboard Requirements

**Pages:**

1. **Overview Dashboard**
   - Current risk score (gauge chart)
   - Current premium vs traditional
   - Recent trips summary
   - Quick stats: miles driven, harsh events, savings

2. **Driving Behavior**
   - Risk score trend (line chart)
   - Harsh events over time
   - Speed distribution
   - Time-of-day heatmap
   - Map view of recent trips

3. **Trip History**
   - List of all trips
   - Trip details: route map, events, score
   - Filters: date range, risk level, time of day

4. **Savings Calculator**
   - Premium breakdown
   - Potential savings projections
   - What-if scenarios
   - Improvement recommendations

5. **Insights & Recommendations**
   - Personalized driving tips
   - Goal tracking
   - Achievement badges
   - Community comparison (anonymized)

#### 7.2 Frontend Technology

**React Component Structure:**

```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Header.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Sidebar.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Card.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Loading.jsx
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RiskScoreGauge.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TripSummary.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SavingsCard.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ QuickStats.jsx
‚îÇ   ‚îú‚îÄ‚îÄ driving/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RiskTrendChart.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HarshEventsChart.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TimeHeatmap.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TripMap.jsx
‚îÇ   ‚îú‚îÄ‚îÄ trips/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TripList.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TripDetails.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TripFilters.jsx
‚îÇ   ‚îî‚îÄ‚îÄ pricing/
‚îÇ       ‚îú‚îÄ‚îÄ PremiumBreakdown.jsx
‚îÇ       ‚îú‚îÄ‚îÄ SavingsProjection.jsx
‚îÇ       ‚îî‚îÄ‚îÄ WhatIfCalculator.jsx
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx
‚îÇ   ‚îú‚îÄ‚îÄ DrivingBehavior.jsx
‚îÇ   ‚îú‚îÄ‚îÄ Trips.jsx
‚îÇ   ‚îú‚îÄ‚îÄ Pricing.jsx
‚îÇ   ‚îî‚îÄ‚îÄ Profile.jsx
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ api.js
‚îÇ   ‚îî‚îÄ‚îÄ auth.js
‚îú‚îÄ‚îÄ hooks/
‚îÇ   ‚îú‚îÄ‚îÄ useRiskScore.js
‚îÇ   ‚îú‚îÄ‚îÄ useTrips.js
‚îÇ   ‚îî‚îÄ‚îÄ usePricing.js
‚îú‚îÄ‚îÄ contexts/
‚îÇ   ‚îî‚îÄ‚îÄ AuthContext.jsx
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ formatters.js
    ‚îî‚îÄ‚îÄ validators.js
```

#### 7.3 Key Visualizations

Implement using Recharts or Chart.js:

1. **Risk Score Gauge:**
   - Semi-circular gauge showing 0-100 score
   - Color-coded zones (green/yellow/orange/red)
   - Needle pointing to current score

2. **Risk Trend Line Chart:**
   - Last 30/90/365 days
   - Moving average overlay
   - Event markers for incidents

3. **Time-of-Day Heatmap:**
   - Hour of day (0-23) √ó Day of week
   - Color intensity = risk level or frequency
   - Identify risky driving patterns

4. **Trip Map:**
   - Leaflet/Mapbox integration
   - Route visualization with color-coded segments
   - Event markers (harsh brake/accel/speeding)

5. **Premium Comparison:**
   - Side-by-side bar chart
   - Traditional vs telematics premium
   - Breakdown by component

### 8. IMPLEMENTATION PHASES

#### Phase 1: Foundation (Weeks 1-3)

**Week 1:**
- Set up project structure and version control
- Configure Docker Compose for local development
- Set up Kafka cluster (3 brokers)
- Set up PostgreSQL and Redis
- Create database schema
- Implement basic FastAPI skeleton

**Week 2:**
- Implement telematics data simulator
- Create Kafka producers/consumers
- Implement Avro schemas
- Set up basic streaming pipeline (Spark Structured Streaming)
- Implement data quality checks

**Week 3:**
- Set up Airflow
- Create first DAG (trip aggregation)
- Implement feature engineering for ML
- Set up S3/MinIO for data lake
- Basic data visualization

**Deliverables:**
- Working data pipeline (end-to-end)
- 1000 simulated drivers with 7 days of data
- Basic API for data ingestion

#### Phase 2: ML & Analytics (Weeks 4-6)

**Week 4:**
- Feature engineering pipeline
- Create training dataset
- Train baseline risk scoring model
- Evaluate model performance
- Set up MLflow for experiment tracking

**Week 5:**
- Implement SHAP explanations
- Hyperparameter tuning
- Model versioning and registry
- Batch inference pipeline
- Model monitoring setup

**Week 6:**
- Implement pricing engine
- Premium calculation logic
- Create pricing API endpoints
- Unit tests for pricing logic
- Integration with risk model

**Deliverables:**
- Trained risk scoring model (R¬≤ > 0.70)
- Working pricing engine
- API endpoints for risk & pricing

#### Phase 3: Frontend & Integration (Weeks 7-9)

**Week 7:**
- React app setup
- Authentication flow
- Dashboard layout
- API client integration

**Week 8:**
- Implement all dashboard components
- Data visualization components
- Trip history and details
- Premium calculator

**Week 9:**
- Polish UI/UX
- Add responsiveness
- Error handling
- Loading states
- End-to-end testing

**Deliverables:**
- Complete user dashboard
- Mobile-responsive design
- Full integration with backend

#### Phase 4: Production Readiness (Weeks 10-12)

**Week 10:**
- Comprehensive testing (unit, integration, E2E)
- Load testing (simulate 10,000 concurrent users)
- Security audit and fixes
- GDPR compliance implementation

**Week 11:**
- Monitoring setup (Prometheus + Grafana)
- Logging (ELK stack)
- Alerting rules
- Documentation (API docs, user guide, ops manual)

**Week 12:**
- Performance optimization
- Cost optimization
- Deployment automation (CI/CD)
- Final testing and bug fixes

**Deliverables:**
- Production-ready system
- Complete documentation
- Deployment runbooks

### 9. TESTING REQUIREMENTS

#### 9.1 Unit Tests

Test coverage target: >80%

**Key areas:**
- Feature engineering functions
- Risk scoring calculations
- Pricing calculations
- API endpoint logic
- Data validation

```python
# Example test structure
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_feature_engineering.py
‚îÇ   ‚îú‚îÄ‚îÄ test_risk_scoring.py
‚îÇ   ‚îú‚îÄ‚îÄ test_pricing_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ test_api_validators.py
‚îÇ   ‚îî‚îÄ‚îÄ test_simulators.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_kafka_integration.py
‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py
‚îÇ   ‚îú‚îÄ‚îÄ test_database_operations.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ml_pipeline.py
‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îú‚îÄ‚îÄ test_data_flow.py
‚îÇ   ‚îú‚îÄ‚îÄ test_user_journeys.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pricing_flow.py
‚îî‚îÄ‚îÄ performance/
    ‚îú‚îÄ‚îÄ test_api_load.py
    ‚îú‚îÄ‚îÄ test_streaming_throughput.py
    ‚îî‚îÄ‚îÄ test_batch_performance.py
```

#### 9.2 Integration Tests

Test complete flows:
- Telematics event ‚Üí Kafka ‚Üí Stream processing ‚Üí Feature store
- Feature store ‚Üí ML model ‚Üí Risk score ‚Üí Pricing
- API request ‚Üí Database ‚Üí Response
- Batch job ‚Üí Data lake ‚Üí ML training

#### 9.3 Performance Tests

Use Locust or k6 for load testing:

**Targets:**
- API response time: p95 < 200ms
- Streaming latency: < 5 seconds end-to-end
- Batch processing: 1M events in < 30 minutes
- Concurrent users: Support 10,000 simultaneous

#### 9.4 Data Quality Tests

Implement Great Expectations:

```python
# Example expectations
expect_column_values_to_not_be_null("driver_id")
expect_column_values_to_be_between("speed", min_value=0, max_value=150)
expect_column_values_to_be_between("latitude", min_value=-90, max_value=90)
expect_column_values_to_be_in_set("event_type", 
    ["normal", "harsh_brake", "rapid_accel", "speeding"])
```

### 10. MONITORING & OBSERVABILITY

#### 10.1 Metrics to Track

**System Metrics:**
- Kafka lag per consumer group
- Spark streaming batch duration
- API request rate and latency
- Database connection pool usage
- Redis hit/miss ratio
- Airflow DAG success rate

**Business Metrics:**
- Events ingested per second
- Drivers with active devices
- Average risk score across fleet
- Premium adjustments (increase/decrease)
- Customer engagement (dashboard logins)

**ML Metrics:**
- Model prediction latency
- Prediction distribution
- Feature drift
- Model accuracy over time

#### 10.2 Alerting Rules

Set up alerts for:
- Kafka lag > 1000 messages
- API error rate > 1%
- Streaming job failure
- Batch job failure or delay
- Model prediction anomalies
- Database connection failures
- High memory/CPU usage

#### 10.3 Logging Strategy

**Log Levels:**
- DEBUG: Detailed diagnostic info
- INFO: General informational messages
- WARNING: Warning messages (deprecated features)
- ERROR: Error events (handled exceptions)
- CRITICAL: Critical errors (system failure)

**Structured Logging:**
```python
import structlog

logger = structlog.get_logger()

logger.info(
    "risk_score_calculated",
    driver_id=driver_id,
    risk_score=risk_score,
    model_version="v1.2.0",
    processing_time_ms=processing_time
)
```

### 11. SECURITY & COMPLIANCE

#### 11.1 Data Security

**Encryption:**
- TLS 1.3 for all API communication
- Encryption at rest for S3 (AES-256)
- Encrypted Kafka topics
- Database encryption (PostgreSQL)

**Access Control:**
- Role-Based Access Control (RBAC)
- API key rotation policy (90 days)
- Principle of least privilege
- Audit logging for all access

#### 11.2 Privacy Compliance

**GDPR Requirements:**
- Right to access: API endpoint for data export
- Right to deletion: Soft delete with purge schedule
- Right to portability: JSON/CSV export
- Consent management: Opt-in/opt-out tracking
- Data minimization: Only collect necessary data
- Retention policy: 7 years for insurance data

**Data Anonymization:**
- PII encryption in analytics datasets
- Differential privacy for aggregate statistics
- k-anonymity for public dashboards

#### 11.3 Regulatory Compliance

**Insurance Regulations:**
- Model explainability (SHAP values)
- Adverse action notices (premium increases)
- Rate filing documentation
- Anti-discrimination testing
- Audit trail for all decisions

### 12. DOCUMENTATION REQUIREMENTS

#### 12.1 Technical Documentation

Create comprehensive docs:

1. **System Architecture Document**
   - Component diagrams
   - Data flow diagrams
   - Deployment architecture
   - Scalability considerations

2. **API Documentation**
   - OpenAPI/Swagger specs
   - Authentication guide
   - Rate limiting policies
   - Example requests/responses

3. **Database Schema**
   - ERD diagrams
   - Table descriptions
   - Indexes and constraints
   - Migration scripts

4. **ML Model Documentation**
   - Model card (purpose, performance, limitations)
   - Feature descriptions
   - Training procedure
   - Evaluation metrics
   - Bias testing results

5. **Operations Manual**
   - Deployment procedures
   - Monitoring dashboards
   - Troubleshooting guide
   - Runbooks for common issues
   - Disaster recovery plan

#### 12.2 User Documentation

1. **User Guide**
   - Getting started
   - Dashboard overview
   - Understanding your risk score
   - How pricing works
   - Privacy settings

2. **FAQ**
   - Common questions
   - Troubleshooting
   - Privacy concerns
   - Pricing explanations

### 13. CODE QUALITY STANDARDS

#### 13.1 Python Code Style

- Follow PEP 8
- Use type hints
- Docstrings (Google style)
- Maximum line length: 100
- Use Black for formatting
- Use Pylint for linting (score > 8.0)

```python
from typing import List, Dict, Optional
import pandas as pd

def calculate_risk_features(
    trips: List[Dict],
    lookback_days: int = 30
) -> pd.DataFrame:
    """
    Calculate risk features from trip data.
    
    Args:
        trips: List of trip dictionaries with telematics data
        lookback_days: Number of days to include in calculation
        
    Returns:
        DataFrame with calculated features per driver
        
    Raises:
        ValueError: If trips list is empty or lookback_days < 1
    """
    if not trips:
        raise ValueError("Trips list cannot be empty")
    if lookback_days < 1:
        raise ValueError("lookback_days must be >= 1")
    
    # Implementation
    pass
```

#### 13.2 Git Workflow

- Feature branches: `feature/risk-scoring-model`
- Commit messages: Conventional Commits format
- PR requirements: Code review + tests passing
- Main branch: Protected, no direct commits
- Tags: Semantic versioning (v1.0.0)

#### 13.3 CI/CD Pipeline

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      - name: Run linting
        run: pylint src/
      - name: Run type checking
        run: mypy src/
      - name: Run tests
        run: pytest tests/ --cov=src --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

### 14. CONFIGURATION MANAGEMENT

Use environment-based configuration:

```python
# config.py
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # Application
    APP_NAME: str = "Telematics Insurance System"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False
    
    # Database
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 10
    
    # Kafka
    KAFKA_BOOTSTRAP_SERVERS: str
    KAFKA_CONSUMER_GROUP: str = "telematics-consumers"
    
    # Redis
    REDIS_HOST: str = "localhost"
    REDIS_PORT: int = 6379
    
    # S3
    S3_BUCKET: str
    S3_REGION: str = "us-east-1"
    
    # ML Model
    MODEL_PATH: str = "/models/risk_scoring/v1.0"
    MODEL_VERSION: str = "v1.0"
    
    # API
    API_RATE_LIMIT: int = 100  # requests per minute
    JWT_SECRET_KEY: str
    JWT_ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    
    class Config:
        env_file = ".env"

@lru_cache()
def get_settings():
    return Settings()
```

### 15. DELIVERABLES CHECKLIST

At project completion, ensure you have:

**Code:**
- [ ] Complete source code in Git repository
- [ ] All tests passing (>80% coverage)
- [ ] Linting and type checking passing
- [ ] Docker Compose setup for local development
- [ ] CI/CD pipeline configured

**Data Pipeline:**
- [ ] Kafka cluster operational
- [ ] Streaming pipeline processing events
- [ ] Batch jobs running on schedule
- [ ] Data lake with historical data
- [ ] Feature store operational

**ML Models:**
- [ ] Trained risk scoring model (R¬≤ > 0.70)
- [ ] Model explainability (SHAP)
- [ ] Model registry with versioning
- [ ] Inference pipeline
- [ ] Model monitoring

**API:**
- [ ] All endpoints implemented and tested
- [ ] Authentication working
- [ ] Rate limiting configured
- [ ] API documentation (Swagger)
- [ ] Error handling

**Frontend:**
- [ ] Dashboard fully functional
- [ ] All visualizations working
- [ ] Mobile responsive
- [ ] Loading and error states
- [ ] User authentication

**Documentation:**
- [ ] README with setup instructions
- [ ] Architecture documentation
- [ ] API documentation
- [ ] User guide
- [ ] Operations manual

**Testing:**
- [ ] Unit tests (>80% coverage)
- [ ] Integration tests
- [ ] E2E tests
- [ ] Load testing results
- [ ] Security testing

**Deployment:**
- [ ] Local deployment working
- [ ] Production deployment guide
- [ ] Environment configuration
- [ ] Secrets management
- [ ] Monitoring dashboards

## SPECIFIC IMPLEMENTATION NOTES

### Priority 1 (Core Features)
1. Data simulation and ingestion
2. Stream processing pipeline
3. Feature engineering
4. Risk scoring model
5. Basic API endpoints
6. Simple dashboard

### Priority 2 (Enhanced Features)
1. Batch processing (Airflow)
2. External data enrichment
3. Pricing engine
4. Advanced visualizations
5. Model monitoring

### Priority 3 (Production Features)
1. Comprehensive testing
2. Security hardening
3. Performance optimization
4. Full documentation
5. Deployment automation

## SUCCESS CRITERIA

The project is successful if:

1. **Technical:**
   - System processes 10,000+ events/second
   - API latency p95 < 200ms
   - Model R¬≤ > 0.70
   - System uptime > 99.5%

2. **Business:**
   - Risk scores correlate with claims (>0.60)
   - 30-40% of drivers receive discounts
   - Premium accuracy improves vs traditional
   - Positive user feedback

3. **Code Quality:**
   - Test coverage > 80%
   - No critical security vulnerabilities
   - Documentation complete
   - Follows all style guidelines

## ADDITIONAL CONSIDERATIONS

### Scalability Plan
- Current: Support 10,000 drivers
- Phase 2: Scale to 100,000 drivers
- Phase 3: Scale to 1,000,000+ drivers

### Cost Considerations
- Optimize Kafka retention (7 days raw, 30 days processed)
- Use S3 lifecycle policies (move to Glacier after 1 year)
- Right-size compute resources
- Use spot instances for batch processing

### Future Enhancements
- Mobile app for real-time feedback
- Gamification features
- Social comparison
- Predictive maintenance alerts
- Crash detection and emergency response
- Weather-based dynamic pricing
- Route optimization suggestions

---

## FINAL NOTES

This is a comprehensive system that touches multiple domains: distributed systems, machine learning, backend development, frontend development, and data engineering. 

Focus on building a solid MVP first (Phase 1 + Priority 1 features), then iteratively add complexity.

Remember:
- **Start simple**: Working end-to-end pipeline first
- **Test early**: Don't accumulate technical debt
- **Document as you go**: Don't leave it for the end
- **Monitor everything**: You can't improve what you don't measure
- **Think about scale**: Design for 10x growth from day one

Good luck with the implementation! üöÄ
